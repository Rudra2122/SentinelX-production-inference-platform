# âš™ï¸ SentinelX â€” Real-Time AI Inference & Infrastructure Platform
## ğŸš€ Executive Summary

SentinelX is a production-grade, real-time AI inference platform built to serve PyTorch models at scale with dynamic batching, autoscaling, health-aware routing, strict SLO enforcement, and deep observability.

Architecturally inspired by NVIDIA Triton, Google Vertex AI, AWS SageMaker Endpoints, and large-scale internal serving systems at Meta, Uber, and OpenAI, SentinelX focuses on the hard infrastructure problems of modern ML systems â€” not just model accuracy.


## ğŸ§  Why This Project Matters

At companies like Meta, NVIDIA, and Google, the hardest ML problems are not model accuracy.

They are:

-  Tail latency under burst traffic

-  Silent failures when workers die

-  Inefficient GPU utilization

-  Unsafe rollouts of new model versions

-  Lack of observability when something breaks at 2 AM

-  SentinelX directly targets these problems with a production-first design.

## ğŸ—ï¸ High-Level Architecture
```
Client
  â”‚
  â–¼
API Gateway (FastAPI / gRPC)
  â”‚
  â–¼
Request Router
  â”œâ”€â”€ SLO enforcement (p95 latency, error budget)
  â”œâ”€â”€ Health-aware routing
  â”œâ”€â”€ A/B & Canary version selection
  â”œâ”€â”€ Backpressure control
  â”‚
  â–¼
Job Scheduler (Redis)
  â”‚
  â–¼
Inference Workers (CPU / GPU)
  â”œâ”€â”€ Dynamic batching (configurable)
  â”œâ”€â”€ Concurrency control
  â”œâ”€â”€ PyTorch execution
  â”œâ”€â”€ Heartbeats w/ TTL
  â”‚
  â–¼
Result Store (Redis)
  â”‚
  â–¼
Streaming Results (SSE)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Observability Plane (always-on):
Prometheus â€¢ Grafana â€¢ OpenTelemetry â€¢ Alerts
```

## ğŸŒŸ Core Features â€” With Real Numbers
### âš¡ 1. High-Throughput Model Serving

-  REST + async inference APIs

-  Optional gRPC endpoint (Triton-style)

-  Supports real-time and batch-style inference

-  Measured throughput

-  300+ requests/sec

-  Sustained under burst load

-  No request loss during autoscaling

### ğŸ“¦ 2. Dynamic Batching (Throughput Without Killing Latency)

-  Per-model configurable:

-  max_batch_size

-  max_batch_wait_ms

-  Measured impact

Batch Size	Throughput
1	          ~80 RPS
16	        ~220 RPS (+175%)

-  Latency remained under SLO (p99 < 200 ms).

### ğŸ“ˆ 3. Autoscaling Worker Pool (Control Plane)

Scales workers up and down automatically

Driven by:

-  Queue depth

-  Worker heartbeat health

-  Cooldown windows

-  Observed behavior

-  Burst: 10 â†’ 200 RPS

-  Workers: 2 â†’ 8 in < 30 seconds

-  0 SLO violations during scale-up

-  Scales back down when load drops

### ğŸ©º 4. Health-Aware Routing & Fail-Fast Guarantees

-  Workers emit heartbeats with TTL

-  Gateway enforces pre-enqueue health checks

-  If no healthy workers exist:

-  Requests fail immediately with HTTP 503

-  No silent queue buildup

-  Tested failure modes

-  Worker crashes mid-load

-  All workers unavailable

-  Redis heartbeat expiration

### ğŸ”€ 5. Multi-Version Routing (A/B + Canary)

-  Serve multiple versions of a model concurrently

-  Deterministic canary bucketing

-  Safe traffic shifting without restarts

-  Example

   v1.0.0 (primary)

   v1.1.0 (canary at 20%)

-  Automatic fallback if canary unhealthy

### ğŸš¦ 6. SLO Enforcement & Backpressure

-  Explicit SLOs enforced in the routing layer:

-  p95 latency target (e.g., < 200 ms)

-  Error rate budget (< 1%)

-  Behavior:

   Traffic is rejected early when system health degrades

   Backpressure prevents cascading failures

### ğŸ“Š 7. Full Observability (Production-First)

Metrics (40+ exposed)

p50 / p95 / p99 latency

Throughput (RPS)

Queue depth

Worker health

Error rate

GPU utilization & memory (when available)

Tracing

End-to-end request traces via OpenTelemetry

Latency breakdown across gateway, queue, and worker

Alerts

Prometheus alerts fire on:

High p95 latency

Error rate violations

No healthy workers

### ğŸ§  8. GPU Metrics (When Available)

GPU utilization %

GPU memory used / total

Exported directly via Prometheus (NVML)

This enables:

GPU saturation analysis

Batch-size tuning

Cost/performance tradeoffs

â±ï¸ Latency Distribution (Measured)

p50: ~40 ms

p90: ~85 ms

p99: < 200 ms

Maintained under burst traffic and autoscaling.

ğŸ§ª Failure Scenarios Explicitly Tested

Worker crashes during inference

Queue overload

SLO violations

Autoscaler recovery

Heartbeat expiration

Result

No stuck requests

No silent data loss

System self-heals

ğŸ§° Tech Stack
Layer	            Technology
Language	        Python
API	                FastAPI + gRPC
Inference	        PyTorch
Queue	            Redis
Infrastructure	    Docker, Docker Compose
Autoscaling         Custom control-plane daemon
Observability	    rometheus, Grafana, OpenTelemetry
GPU Metrics	        NVML (when available)

## ğŸ§© Repository Structure
```
sentinelx/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py              # FastAPI gateway
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ router.py            # Routing, SLOs, health enforcement
â”‚   â”œâ”€â”€ autoscaler.py        # Worker autoscaling logic
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ worker.py            # Inference worker loop
â”‚   â””â”€â”€ loader.py            # Model loading & warmup
â”œâ”€â”€ registry/
â”‚   â””â”€â”€ registry.py          # Model registry & traffic control
â”œâ”€â”€ observability/
â”‚   â”œâ”€â”€ metrics.py           # Prometheus metrics
â”‚   â””â”€â”€ logging.py
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile.api
â”‚   â”œâ”€â”€ Dockerfile.worker
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ load_test_async.py
â”‚   â””â”€â”€ plot_benchmarks.py
â””â”€â”€ docs/
    â””â”€â”€ bench/               # Benchmark graphs (PNG)
```

```
âš¡ Quick Start
docker compose up --build
```


## Access

Gateway: http://localhost:8000

Prometheus: http://localhost:9090

Grafana: http://localhost:3000

## ğŸ§­ Whatâ€™s Next

Kubernetes deployment

GPU-aware scheduling

Model warm-swap without downtime

Multi-tenant isolation

Cost-aware autoscaling

ğŸ‘¤ Author

Rudra
Software Engineer Â· Distributed Systems Â· AI Infrastructure
